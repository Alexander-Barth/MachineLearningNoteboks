{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Discriminant Analysis\n",
    "\n",
    "* Assume that we have a set of vectors $\\vec x^{(i)}$ and for each vector we have a label $y_i$ which is either 0 or 1.\n",
    "\n",
    "Means and variances for each class\n",
    "\n",
    "for $y = 0$\n",
    "\n",
    "$$\n",
    "\\vec \\mu_0 = \\frac{1}{n_0} \\sum_{i \\in C_0} \\vec x^{(i)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Sigma_0 =  \\frac{1}{n_0-1} \\sum_{i \\in C_0} (\\vec x^{(i)} - \\vec \\mu_0)^T (\\vec x^{(i)} - \\vec \\mu_0)\n",
    "$$\n",
    "\n",
    "and for  $y = 1$\n",
    "\n",
    "$$\n",
    "\\vec \\mu_1 = \\frac{1}{n_1} \\sum_{i \\in C_1} \\vec x^{(i)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Sigma_1 =  \\frac{1}{n_1-1} \\sum_{i \\in C_1} (\\vec x^{(i)} - \\vec \\mu_1)^T (\\vec x^{(i)} - \\vec \\mu_1)\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\newcommand{\\mat}{\\mathbf}\n",
    "\\renewcommand{\\vec}{\\mathbf}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\vec w = \\mathrm{argmax}_w \\frac{ (\\vec \\mu_0^T \\vec w - \\vec \\mu_1^T \\vec w)^2 } \n",
    "{n_0 \\vec w^T \\mat \\Sigma_0  \\vec w + n_1 \\vec w^T \\mat \\Sigma_1  \\vec w}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\frac{ ((\\vec \\mu_O  - \\vec \\mu_1)^T \\vec w)^2 } \n",
    "{\\vec w^T (n_0  \\mat \\Sigma_0  + n_1  \\mat \\Sigma_1)  \\vec w}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\frac{ (\\vec m^T \\vec w)^2 } \n",
    "{\\vec w^T \\mat S  \\vec w}\n",
    "$$\n",
    "\n",
    "$\\mat S = (n_0  \\mat \\Sigma_0  + n_1  \\mat \\Sigma_1) $ and $\\vec m = \\vec \\mu_O  - \\vec \\mu_1$.\n",
    "Just as $\\Sigma_0$  and $\\mat \\Sigma_1$, $\\mat S$ is a symetric positive defined matrix.\n",
    "\n",
    "$$\n",
    "= \\frac{ (\\vec m^T \\vec w)^2 } \n",
    "{\\vec w^T \\mat R^T \\mat R  \\vec w}\n",
    "$$\n",
    "\n",
    "where $\\mat S = \\mat R^T \\mat R$ (square root decomposition)\n",
    "\n",
    "\n",
    "$$\n",
    "= \\frac{ (\\vec m^T \\mat R^{-1} \\vec v)^2 } \n",
    "{\\vec v^T {\\mat R^T}^{-1} \\mat R^T \\mat R  \\mat R^{-1} \\vec v}\n",
    "$$\n",
    "\n",
    "where $\\vec v = \\mat R \\vec w$ (transformed vector)\n",
    "\n",
    "$$\n",
    "= \\frac{ (\\vec m^T \\mat R^{-1} \\vec v)^2 } \n",
    "{\\vec v^T  \\vec v}\n",
    "$$\n",
    "\n",
    "$\\vec v^T  \\vec v = \\| \\vec v \\|^2$\n",
    "\n",
    "$$\n",
    "= \\left( \\frac{ \\vec m^T \\mat R^{-1} \\vec v} \n",
    "{\\| \\vec v \\|} \\right)^2\n",
    "= \n",
    "\\left( \\frac{ ({\\mat R^{-1}}^T \\vec m)^T  \\vec v} \n",
    "{\\| \\vec v \\|} \\right)^2\n",
    "$$\n",
    "\n",
    "The function to maximize could be rewritten as the project between two vectors. The projection is maximum if they are a multiple of each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\vec v = a \\; {\\mat R^{-1}}^T \\vec m = a; {\\mat R^{-1}}^T (\\vec \\mu_0 - \\vec \\mu_1)\n",
    "$$\n",
    "\n",
    "where $a$ is a arbitrary scalar.\n",
    "\n",
    "$$\n",
    "\\vec w = \\mat R^{-1} \\vec v =  a \\; \\mat R^{-1}  {\\mat R^{-1}}^T (\\vec \\mu_0 - \\vec \\mu_1) = a \\; \\mat S^{-1} (\\vec \\mu_0 - \\vec \\mu_1)\n",
    "$$\n",
    "\n",
    "$$ \n",
    "\\vec w = (n_0  \\mat \\Sigma_0  + n_1  \\mat \\Sigma_1)^{-1} (\\vec \\mu_0 - \\vec \\mu_1)\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.2",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
